{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## 1. Importing Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    StackingRegressor,\n",
    "    VotingRegressor,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. Load The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "We read from the 04-encoded folder we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define paths (using pathlib for robustness)\n",
    "PROJECT_ROOT = Path().resolve().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"04-encoded\"\n",
    "\n",
    "# 2. Load the datasets\n",
    "X_train = pd.read_csv(DATA_DIR / \"X_train_encoded.csv\")\n",
    "X_val = pd.read_csv(DATA_DIR / \"X_val_encoded.csv\")\n",
    "y_train = pd.read_csv(DATA_DIR / \"y_train.csv\").values.ravel()  # ravel() flattens it to an array\n",
    "y_val = pd.read_csv(DATA_DIR / \"y_val.csv\").values.ravel()\n",
    "\n",
    "print(f\"Data Loaded. X_train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 3. Scaling The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### 1\\. The Safety Check (Age Imputation)\n",
    "\n",
    "**Crucial:** If you didn't explicitly fill missing values in the Age column during your EDA/Feature Engineering phase, the Scaler will **crash**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Age has missing values\n",
    "if X_train[\"age\"].isnull().sum() > 0:\n",
    "    print(f\"Found {X_train['age'].isnull().sum()} missing ages. Filling with Median...\")\n",
    "\n",
    "    # Calculate median on TRAIN\n",
    "    age_median = X_train[\"age\"].median()\n",
    "\n",
    "    # Fill on all\n",
    "    X_train[\"age\"] = X_train[\"age\"].fillna(age_median)\n",
    "    X_val[\"age\"] = X_val[\"age\"].fillna(age_median)\n",
    "\n",
    "print(\"âœ… No missing values in Age.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### 2\\. Scale & Save (The MLOps Step)\n",
    "\n",
    "We only scale the continuous columns (age, fare, FamilySize). We do **not** touch the binary columns (like sex\\_male, pclass\\_2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define columns to scale\n",
    "scale_cols = [\"age\", \"fare\", \"familysize\"]\n",
    "\n",
    "# 2. Initialize and Fit Scaler (On TRAIN only)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[scale_cols])\n",
    "\n",
    "# 3. Transform Data\n",
    "# We use .loc to modify the specific columns in place\n",
    "X_train.loc[:, scale_cols] = scaler.transform(X_train[scale_cols])\n",
    "X_val.loc[:, scale_cols] = scaler.transform(X_val[scale_cols])\n",
    "\n",
    "# 4. Save the Scaler (CRITICAL for your API later)\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "joblib.dump(scaler, MODEL_DIR / \"scaler.pkl\")\n",
    "# Save the list of columns that the model expects\n",
    "# This ensures we can align the API input perfectly later\n",
    "joblib.dump(X_train.columns.tolist(), \"../models/model_columns.pkl\")\n",
    "\n",
    "print(\"Model columns saved. We will use this to align the API input.\")\n",
    "\n",
    "print(f\"Data scaled and scaler saved to {MODEL_DIR}/scaler.pkl\")\n",
    "print(X_train[scale_cols].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "califonia_housing_prices",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
