{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## 1. Importing Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import missingno as msno\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.ensemble import (\n",
    "    GradientBoostingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    StackingRegressor,\n",
    "    VotingRegressor,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    \"/home/kobey/Documents/DATASCIENCE/PROJECTS/TITANIC DATASET/data/01-raw/Titanic-Dataset.csv\"\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### 2.1. Make all columns lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to lowercase\n",
    "df = df.rename(columns=str.lower)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 3. Train, Test and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependent and Independent variables/features\n",
    "y = df[\"survived\"]  # target data\n",
    "X = df.drop(\"survived\", axis=1)  # feature data\n",
    "\n",
    "# first split: train + temp\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# second split: validation + test\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Class Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Check if survived is present in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Survived\" in X_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Check that indices don’t overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "set(X_train.index) & set(X_val.index)\n",
    "set(X_train.index) & set(X_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "###  3.1 Save the split up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"../data/02-split/X_train.csv\", index=False)\n",
    "y_train.to_csv(\"../data/02-split/y_train.csv\", index=False)\n",
    "X_val.to_csv(\"../data/02-split/X_val.csv\", index=False)\n",
    "y_val.to_csv(\"../data/02-split/y_val.csv\", index=False)\n",
    "X_test.to_csv(\"../data/02-split/X_test.csv\", index=False)\n",
    "y_test.to_csv(\"../data/02-split/y_test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 4. Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### 4.1 Dealing with Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "#### i. Fill The Missing Ages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a \"Lookup Dictionary\" from the training data\n",
    "# This converts the medians into a simple python dict: {(1, 'male'): 40.0, ...}\n",
    "median_dict = X_train.groupby([\"pclass\", \"sex\"])[\"age\"].median().to_dict()\n",
    "\n",
    "# 2. Define the function that works on a SINGLE ROW\n",
    "\n",
    "\n",
    "def fill_age(row):\n",
    "    # If age is missing (NaN), look it up\n",
    "    if pd.isna(row[\"age\"]):\n",
    "        # Create a key tuple from this row's data\n",
    "        key = (row[\"pclass\"], row[\"sex\"])\n",
    "        # Return the median from our dict\n",
    "        return median_dict.get(key, row[\"age\"])\n",
    "\n",
    "    # If age is already there, keep it\n",
    "    return row[\"age\"]\n",
    "\n",
    "\n",
    "# 3. Apply it to all dataframes\n",
    "# We assign the result back to the 'age' column\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    df[\"age\"] = df.apply(fill_age, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"age\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "#### ii. Handle Cabin Missing Data (The Deck Approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying this approach to X_train, X_val, X_test\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    # Check if 'Cabin' exists to avoid errors (handles case sensitivity manually if needed)\n",
    "    if \"cabin\" in df.columns:\n",
    "        # Extract the first letter (Deck), fill NaN with 'U'\n",
    "        df[\"deck\"] = df[\"cabin\"].str[0].fillna(\"U\")\n",
    "\n",
    "        # Drop the original high-cardinality Cabin Column\n",
    "        df.drop(columns=[\"cabin\"], inplace=True)\n",
    "    else:\n",
    "        print(\"Column 'cabin' not found. Check capitalization!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### iii. Visual on Deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a temporary dataframe that joins X and y together\n",
    "# Make sure to use the correct index so they align\n",
    "plot_df = X_train.copy()\n",
    "plot_df[\"Survived\"] = y_train  # Add the target column back in\n",
    "\n",
    "# 2. Now plot using this new dataframe\n",
    "# Note: Check if your target column is named 'Survived' or 'survived'\n",
    "sns.barplot(x=\"deck\", y=\"Survived\", data=plot_df, order=sorted(plot_df[\"deck\"].unique()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## 5. Save the Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the directory path\n",
    "save_dir = \"../data/03-preprocessed\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 2. Save Features (X)\n",
    "# index=False prevents pandas from adding that annoying 'Unnamed: 0' column\n",
    "X_train.to_csv(f\"{save_dir}/X_train_processed.csv\", index=False)\n",
    "X_val.to_csv(f\"{save_dir}/X_val_processed.csv\", index=False)\n",
    "X_test.to_csv(f\"{save_dir}/X_test_processed.csv\", index=False)\n",
    "\n",
    "# 3. Save Targets (y)\n",
    "# We assume y_train, y_val exist from your train_test_split\n",
    "y_train.to_csv(f\"{save_dir}/y_train.csv\", index=False)\n",
    "y_val.to_csv(f\"{save_dir}/y_val.csv\", index=False)\n",
    "\n",
    "print(f\"Preprocessed data saved to {save_dir}/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## 6. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "#### 6.1 Checking if behaviour of survivors was different from non-survivors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "#### i. pclass - Ticket class, a proxy for socio-economic status (1=1st,2=2nd,3=3rd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good for finding Outliers in pclass and seeing if Rich people survived\n",
    "sns.violinplot(x=\"survived\", y=\"pclass\", data=X_train.join(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "The **violin plot** reveals the \"shape\" and density of the data. It shows you where the largest \"crowds\" of passengers were located within each survival group.\n",
    "\n",
    "### Breakdown of the Findings (Violin Plot)\n",
    "\n",
    "**1\\. Those Who Did Not Survive (survived = 0)**\n",
    "\n",
    "*   **The \"Bulge\" at 3.0:** The widest part of the violin (the \"belly\") is concentrated heavily at the top (Class 3). This represents the massive volume of 3rd-class passengers who perished. Visually, it looks like a heavy weight at the top.\n",
    "    \n",
    "*   **The \"Neck\" at 1.0:** The violin tapers off into a very thin line at Class 1. This shows that while 1st-class passengers died, the density—or frequency—of those deaths was extremely low compared to the other classes.\n",
    "    \n",
    "*   **Distribution Shape:** It is \"top-heavy.\" This indicates a skewed distribution where the majority of the data points are clustered at the lower social status/higher class number.\n",
    "    \n",
    "\n",
    "**2\\. Those Who Survived (survived = 1)**\n",
    "\n",
    "*   **The \"Triple Bulge\":** Unlike the non-survivor plot, the survivor violin often looks more like an hourglass or a more balanced cylinder. There are visible bulges at Class 1, Class 2, and Class 3.\n",
    "    \n",
    "*   **Concentration at 1.0:** You will notice a significant widening at the bottom (Class 1) that simply doesn't exist in the non-survivor plot. This visually represents the high survival rate of 1st-class passengers.\n",
    "    \n",
    "*   **The 3rd Class Paradox:** There is still a bulge at Class 3 for survivors. However, when compared to the non-survivor violin, you can see that a much smaller _proportion_ of the 3rd-class \"mass\" made it into this survivor shape.\n",
    "    \n",
    "\n",
    "### Summary Insights from the Density\n",
    "\n",
    "*   **Density vs. Count:** The violin plot makes it obvious that death was a \"crowded\" event for 3rd class, but survival was a \"distributed\" event across all classes.\n",
    "    \n",
    "*   **Visual Skew:** You can immediately see the \"social gradient.\" The non-survivor violin is essentially a pyramid pointing down (most deaths at the bottom/3rd class), while the survivor violin is much more vertically uniform.\n",
    "    \n",
    "*   **Probability at a Glance:** If you were to pick a random point in the \"0\" violin, you are statistically almost certain to land in the 3rd-class bulge. If you pick a point in the \"1\" violin, your chances are much more evenly spread across the three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "#### ii. Sex - Gender of the passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good for seeing if Females survived more\n",
    "sns.barplot(x=\"sex\", y=\"survived\", data=X_train.join(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "The Titanic survival rate by **Sex** is widely considered the most significant predictor in the entire dataset, even outweighing passenger class. This chart visually tells the story of the \"women and children first\" protocol.\n",
    "\n",
    "### Breakdown of the Findings\n",
    "\n",
    "**1\\. Females (High Survival Density)**\n",
    "\n",
    "*   **Survival Rate:** Approximately **74-75%** of women survived.\n",
    "    \n",
    "*   **Interpretation:** In almost any visualization of this data, the \"Survived\" bar for females will be significantly taller than the \"Did Not Survive\" bar. This reflects the priority given to women during the evacuation.\n",
    "    \n",
    "*   **Observation:** If this were a boxplot or violin plot, the \"1\" (survived) section would be much wider and denser than the \"0\" section.\n",
    "    \n",
    "\n",
    "**2\\. Males (High Fatality Density)**\n",
    "\n",
    "*   **Survival Rate:** Only about **18-19%** of men survived.\n",
    "    \n",
    "*   **Interpretation:** The vast majority of male passengers perished. This is shown by a dominant \"Did Not Survive\" bar. Most men remained on board while lifeboats were filled with women and children.\n",
    "    \n",
    "*   **Observation:** The \"0\" (did not survive) category for males usually represents the largest single group in the entire dataset.\n",
    "    \n",
    "\n",
    "### Summary Insights\n",
    "\n",
    "*   **The Gender Gap:** The disparity is stark. Gender was a life-or-death factor, creating a clear \"survival bias\" toward female passengers.\n",
    "    \n",
    "*   **Predictive Strength:** Because this gap is so large, any machine learning model you build will likely identify Sex as the most important feature.\n",
    "    \n",
    "*   **Intersectionality:** While women had a higher survival rate overall, it's worth noting that a 1st-class male still had a lower survival probability than a 3rd-class female—further proving that gender was the primary filter for survival."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "#### iii. Age - Age in years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good for finding Outliers in age and seeing if it determined survival rate\n",
    "sns.violinplot(x=\"survived\", y=\"age\", data=X_train.join(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "Age vs Survival — Key Insights (Titanic Dataset)\n",
    "------------------------------------------------\n",
    "\n",
    "This violin plot compares the age distribution of passengers who **did not survive (0)** and those who **survived (1)**.\n",
    "\n",
    "### 1\\. Survival is higher among younger passengers\n",
    "\n",
    "Passengers who survived tend to be **younger on average** than those who did not. The distribution for survivors is denser in the lower age ranges, particularly among children and young adults.\n",
    "\n",
    "**Insight:** Age appears to be an important factor in survival, with younger passengers having a higher chance of surviving.\n",
    "\n",
    "### 2\\. Children had a clear survival advantage\n",
    "\n",
    "The survivor group shows a noticeable concentration at very low ages (roughly 0–10 years), while this concentration is much smaller for non-survivors.\n",
    "\n",
    "**Insight:** This supports the historical evacuation practice of _“women and children first”_ during the disaster.\n",
    "\n",
    "### 3\\. Middle-aged adults were less likely to survive\n",
    "\n",
    "Passengers who did not survive are heavily concentrated in the **20–40 age range**, whereas the survivor distribution is less dense in this interval.\n",
    "\n",
    "**Insight:** Middle-aged adults, especially men, were more likely to be among the fatalities.\n",
    "\n",
    "### 4\\. Older passengers had low survival rates\n",
    "\n",
    "Both distributions thin out at higher ages (60+), but the survivor group thins out more sharply.\n",
    "\n",
    "**Insight:** Survival probability decreases significantly with increasing age.\n",
    "\n",
    "### 5\\. Age alone is informative but not sufficient\n",
    "\n",
    "Although age shows a clear relationship with survival, there is substantial overlap between the two distributions.\n",
    "\n",
    "**Insight:** Age should not be used as a standalone predictor; it is best combined with other variables such as **sex**, **passenger class (Pclass)**, and **fare** for stronger predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "#### iii. SibSp - Number of siblings or spouses aboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=\"survived\", y=\"sibsp\", data=X_train.join(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "Siblings/Spouses (SibSp) vs Survival — Key Insights\n",
    "---------------------------------------------------\n",
    "\n",
    "This violin plot shows the distribution of **siblings/spouses aboard (SibSp)** for passengers who **did not survive (0)** and those who **survived (1)**.\n",
    "\n",
    "### 1\\. Passengers traveling alone were common in both groups\n",
    "\n",
    "Both survivors and non-survivors show a strong concentration at **SibSp = 0**, indicating that many passengers traveled alone.\n",
    "\n",
    "**Insight:** Traveling alone does not guarantee survival or death, but it is the most common case overall.\n",
    "\n",
    "### 2\\. Small family groups had higher survival rates\n",
    "\n",
    "Survivors are more densely concentrated around **SibSp = 1–2**, while non-survivors show less density in this range.\n",
    "\n",
    "**Insight:** Passengers traveling with **one or two siblings/spouses** were more likely to survive than those traveling alone or in large family groups.\n",
    "\n",
    "### 3\\. Large family groups had lower survival rates\n",
    "\n",
    "The non-survivor group exhibits a long tail with higher SibSp values (up to 8), whereas survivors rarely appear beyond **SibSp = 3–4**.\n",
    "\n",
    "**Insight:** Passengers traveling with large families had a lower chance of survival, likely due to coordination difficulties during evacuation.\n",
    "\n",
    "### 4\\. Extremely high SibSp values are mostly associated with non-survivors\n",
    "\n",
    "High SibSp counts are almost exclusively present in the non-survivor distribution.\n",
    "\n",
    "**Insight:** Very large sibling/spouse groups faced a significant survival disadvantage.\n",
    "\n",
    "### 5\\. SibSp shows a non-linear relationship with survival\n",
    "\n",
    "Survival is highest for **small family sizes**, not for zero or very large values.\n",
    "\n",
    "**Insight:** SibSp is a useful feature, but its effect is **non-monotonic**, meaning simple linear assumptions may miss important patterns.\n",
    "\n",
    "### Notebook takeaway\n",
    "\n",
    "> **Passengers traveling with a small number of siblings or spouses (1–2) had better survival outcomes, while those traveling alone or in large family groups were less likely to survive.**\n",
    "\n",
    "### Feature engineering note (optional but strong)\n",
    "\n",
    "*   Consider creating a **FamilySize = SibSp + Parch + 1** feature.\n",
    "    \n",
    "*   Bin family size into categories such as:\n",
    "    \n",
    "    *   Alone\n",
    "        \n",
    "    *   Small (2–4)\n",
    "        \n",
    "    *   Large (5+)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "#### iv. Parch -  Number of parents or children aboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=\"survived\", y=\"parch\", data=X_train.join(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "\n",
    "### 1.Most passengers were alone\n",
    "\n",
    "*   For both survivors (1) and non-survivors (0), the widest part of the violins is at **parch = 0**\n",
    "    \n",
    "*   This means **most people had no parents or children onboard**\n",
    "    \n",
    "\n",
    "**Interpretation:**Being alone was the norm on the Titanic.\n",
    "\n",
    "### 2.Having parents/children is more common among survivors\n",
    "\n",
    "*   The **survivor group** shows noticeably more density at:\n",
    "    \n",
    "    *   parch = 1\n",
    "        \n",
    "    *   parch = 2\n",
    "        \n",
    "*   The non-survivor group thins out faster above parch = 0\n",
    "    \n",
    "\n",
    "**Interpretation:**Passengers traveling with parents or children were **more likely to survive** than those traveling alone.\n",
    "\n",
    "This aligns with:\n",
    "\n",
    "*   evacuation priorities\n",
    "    \n",
    "*   families staying together\n",
    "    \n",
    "*   children being prioritized\n",
    "    \n",
    "\n",
    "### 3\\. Very large families are rare and noisy\n",
    "\n",
    "*   Both groups have long, thin tails at higher parch values\n",
    "    \n",
    "*   These cases are **very few**\n",
    "    \n",
    "\n",
    "**Interpretation:**High parch values exist, but:\n",
    "\n",
    "*   they won’t strongly influence the model\n",
    "    \n",
    "*   treating them as-is is fine, but don’t overemphasize them\n",
    "    \n",
    "\n",
    "### 4\\. parch alone is not a strong separator\n",
    "\n",
    "*   There is **overlap** between the two violins\n",
    "    \n",
    "*   “If parch > X, then survived”\n",
    "    \n",
    "\n",
    "**Interpretation:**parch has **signal**, but it’s not a decisive feature on its own."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "#### iv. Fare - The fare the passenger paid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=\"survived\", y=\"fare\", data=X_train.join(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62",
   "metadata": {},
   "source": [
    "**Fare vs Survival:**The violin plot shows that passengers who survived generally paid higher fares than those who did not. Survivors exhibit a wider fare distribution with a long upper tail, indicating that high-fare passengers were more likely to survive. In contrast, non-survivors are concentrated at lower fare values. This suggests that ticket price—often a proxy for passenger class and socioeconomic status—had a strong influence on survival outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "#### v. Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"embarked\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"embarked\", y=\"survived\", data=X_train.join(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "## Family"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "We are going to take two confusing columns (SibSp and Parch) and turn them into two powerful new features: **FamilySize** and **IsAlone**.\n",
    "\n",
    "### The Raw Data: What do we have?\n",
    "\n",
    "Currently, your dataset has these two columns:\n",
    "\n",
    "*   **SibSp**: The number of **Sib**lings (brothers/sisters) + **Sp**ouses (husband/wife) on board.\n",
    "    \n",
    "*   **Parch**: The number of **Par**ents + **Ch**ildren on board.\n",
    "    \n",
    "\n",
    "**The Problem:**Imagine a passenger named \"John.\"\n",
    "\n",
    "*   He has 1 wife with him (SibSp = 1).\n",
    "    \n",
    "*   He has 2 kids with him (Parch = 2).\n",
    "    \n",
    "\n",
    "To the model, these are just separate numbers. But to us humans, we know **John is traveling with a family of 4 people.** That is the \"hidden\" information we need to teach the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69",
   "metadata": {},
   "source": [
    "### Step 1: Create FamilySize\n",
    "\n",
    "We want to know the total number of people in a passenger's group.\n",
    "\n",
    "**The Math:**\n",
    "\n",
    "FamilySize=SibSp+Parch+1\n",
    "\n",
    "**Wait, why +1?** Because SibSp and Parch count the _relatives_. You need to add **+1** to include the passenger themselves!\n",
    "\n",
    "**The Code:** Run this block. We apply it to X\\_train, X\\_val, and X\\_test so all your data stays consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 'FamilySize' column\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    df[\"familysize\"] = df[\"sibsp\"] + df[\"parch\"] + 1\n",
    "\n",
    "# Let's verify it worked by looking at the first 5 rows\n",
    "print(X_train[[\"sibsp\", \"parch\", \"familysize\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "### Step 2: Create IsAlone\n",
    "\n",
    "This is a \"Boolean\" (True/False) feature.\n",
    "\n",
    "**The Hypothesis:** People traveling alone were often young men looking for work (who were likely to die). People traveling with families were often women/children or wealthy fathers (who were likely to survive).\n",
    "\n",
    "**The Logic:** If FamilySize is equal to 1, then the person is alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [X_train, X_val, X_test]:\n",
    "    df[\"isalone\"] = (df[\"familysize\"] == 1).astype(int)\n",
    "\n",
    "# verify\n",
    "print(X_train[[\"familysize\", \"isalone\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "### Step 3: Cleanup (The \"Drop\")\n",
    "\n",
    "Now that we have FamilySize, the original SibSp and Parch columns are redundant. They are just \"noise\" now. It is cleaner to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the original columns to reduce clutter\n",
    "cols_to_drop = [\"sibsp\", \"parch\", \"FamilySize\"]\n",
    "\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    df.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(\"Columns dropped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "## Fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "fare = X_train[\"fare\"]\n",
    "mu, sigma = fare.mean(), fare.std()\n",
    "\n",
    "x = np.linspace(fare.min(), fare.max(), 100)\n",
    "pdf = norm.pdf(x, mu, sigma)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(fare, bins=30, density=True)\n",
    "plt.plot(x, pdf)\n",
    "plt.xlabel(\"Fare\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\"Fare vs Normal Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "stats.probplot(fare, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot for Fare\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "### The Problem: \"The Titanic Wealth Gap\"\n",
    "\n",
    "If you plot the fares, you will see that:\n",
    "\n",
    "1.  **95% of passengers** paid less than $100.\n",
    "    \n",
    "2.  **A tiny handful** paid huge amounts (up to $512).\n",
    "    \n",
    "\n",
    "To a machine learning model, that $512 looks like a massive outlier/error compared to the $8 average. It stretches the math too far. We need to \"squash\" these huge numbers down so the model can learn better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "### Step 1: Fill Missing Values (Safety Check)\n",
    "\n",
    "Before we do any math, we must ensure there are no empty slots. In the Titanic dataset, the Test set often has **one** passenger with a missing fare.\n",
    "\n",
    "**The Fix:** Fill it with the median (the middle value). We use the median because the mean (average) is messed up by those rich billionaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the median on TRAIN data only (to avoid cheating)\n",
    "fare_median = X_train[\"fare\"].median()\n",
    "\n",
    "# 2. Fill missing value in all datasets\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    df[\"fare\"] = df[\"fare\"].fillna(fare_median)\n",
    "\n",
    "print(f\"Filled missing fares with median: {fare_median}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82",
   "metadata": {},
   "source": [
    "### Step 2: The Log Transformation (The \"Squash\")\n",
    "\n",
    "Now we fix the skewness. We will use a mathematical trick called a **Logarithm**.\n",
    "\n",
    "*   It leaves small numbers (like $8) mostly alone.\n",
    "    \n",
    "*   It aggressively shrinks huge numbers (like $512 becomes ~6.2).\n",
    "    \n",
    "\n",
    "**Crucial Math Note:** You cannot take the Log of 0. Some people on the Titanic paid $0 (crew or special guests).Therefore, we use np.log1p which means **\"Log of (1 + Fare)\"**. This prevents the \"Divide by Zero\" crash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the log transformation to all datesets\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    # This overwrites the old fare with the new 'log-transformed fare'\n",
    "    df[\"fare\"] = np.log1p(df[\"fare\"])\n",
    "\n",
    "print(\"Fare column successfully log-transformed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "### Step 3: Verify the Fix (Visual Check)\n",
    "\n",
    "You should always trust but verify. Let's look at what we just did.\n",
    "\n",
    "Run this code to compare the \"Before\" (which you can't see anymore, but imagine it) vs the \"After\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the new Distribution\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(X_train[\"fare\"], kde=True)\n",
    "plt.title(\"Distribution of Fare after Log Transformation\")\n",
    "plt.xlabel(\"Log(Fare)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "## Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "Right now, the name column looks like this: _\"Braund, Mr. Owen Harris\"_.To a computer, that is just random text. It doesn't know that **\"Mr\"** means an adult man (who likely died) and **\"Master\"** means a little boy (who likely lived).\n",
    "\n",
    "If we skip this, your model will miss out on the strongest predictor of survival for males.\n",
    "\n",
    "Let's Feature Engineer **Titles** from Names step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "### Step 1: Extract the Title\n",
    "\n",
    "We need to pull out the word that comes after the comma and ends with a dot (e.g., Mr., Mrs., Dr.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Extract the title from the Name column\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    # The regex ' ([A-Za-z]+)\\.' looks for words ending in a dot\n",
    "    df[\"title\"] = df[\"name\"].str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n",
    "\n",
    "# Let's see what we found\n",
    "print(X_train[\"title\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "### Step 2: Group the \"Rare\" Titles\n",
    "\n",
    "Having a category with only 1 person (like \"Jonkheer\") confuses the model. We need to group these rare titles into a single category called **\"Rare\"** (or map them to their closest standard title).\n",
    "\n",
    "**The Strategy:**\n",
    "\n",
    "*   **Mr, Miss, Mrs, Master:** Keep these.\n",
    "    \n",
    "*   **Mlle, Ms:** Convert to **Miss**.\n",
    "    \n",
    "*   **Mme:** Convert to **Mrs**.\n",
    "    \n",
    "*   **Dr, Rev, Col, Major, Don, etc.:** Group as **Rare**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping dictionary\n",
    "title_mapping = {\n",
    "    \"Mr\": \"Mr\",\n",
    "    \"Miss\": \"Miss\",\n",
    "    \"Mrs\": \"Mrs\",\n",
    "    \"Master\": \"Master\",\n",
    "    \"Dr\": \"Rare\",\n",
    "    \"Rev\": \"Rare\",\n",
    "    \"Col\": \"Rare\",\n",
    "    \"Major\": \"Rare\",\n",
    "    \"Mlle\": \"Miss\",\n",
    "    \"Countess\": \"Rare\",\n",
    "    \"Ms\": \"Miss\",\n",
    "    \"Lady\": \"Rare\",\n",
    "    \"Jonkheer\": \"Rare\",\n",
    "    \"Don\": \"Rare\",\n",
    "    \"Dona\": \"Rare\",\n",
    "    \"Mme\": \"Mrs\",\n",
    "    \"Capt\": \"Rare\",\n",
    "    \"Sir\": \"Rare\",\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    df[\"title\"] = df[\"title\"].map(title_mapping)\n",
    "    # Fill any we missed with \"Rare\" just in case\n",
    "    df[\"title\"] = df[\"title\"].fillna(\"Rare\")\n",
    "\n",
    "print(\"Titles grouped successfully!\")\n",
    "print(X_train[\"title\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "### Step 3: Drop the original Name column\n",
    "\n",
    "Now that we have the title, the full name is just noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\"name\", \"passengerid\"]  # We also drop PassengerId now as it's useless\n",
    "\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    df.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(\"Name and PassengerId dropped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "## Embarked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "### The Problem: Missing Data\n",
    "\n",
    "In the training set, two passengers are missing their port of embarkation. We can't leave these empty, or the model will crash.\n",
    "\n",
    "### The Solution: \"Follow the Crowd\" (Mode Imputation)\n",
    "\n",
    "Since we only have 2 missing values, the safest statistical guess is to assume they boarded at the most popular port.\n",
    "\n",
    "If you look at the data, the vast majority of people boarded at **Southampton (S)**.\n",
    "\n",
    "Therefore, we will fill the missing spots with 'S'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Find the most common port in the TRAIN set\n",
    "# .mode() returns a series, so we take the first item [0]\n",
    "embarked_mode = X_train[\"embarked\"].mode()[0]\n",
    "\n",
    "print(f\"Most common port is: {embarked_mode}\")\n",
    "\n",
    "# 2. Fill missing values in all datasets\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    df[\"embarked\"] = df[\"embarked\"].fillna(embarked_mode)\n",
    "\n",
    "print(\"Embarked column successfully filled.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "## Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99",
   "metadata": {},
   "source": [
    "#### Step 1: The Logic \n",
    "\n",
    "We want to extract the first part of the ticket.\n",
    "\n",
    "    A/5 21171 → Prefix: A5\n",
    "\n",
    "    PC 17599 → Prefix: PC\n",
    "\n",
    "    113803 (Just numbers) → Prefix: X (No Prefix)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the cleaning function\n",
    "\n",
    "\n",
    "def clean_ticket(ticket):\n",
    "    ticket = str(ticket)\n",
    "    if ticket.isdigit():\n",
    "        return \"X\"  # Represents \"No Prefix\" (just numbers)\n",
    "    else:\n",
    "        # Get the first part (prefix), remove '.' and '/'\n",
    "        prefix = ticket.split(\" \")[0]\n",
    "        prefix = prefix.replace(\".\", \"\").replace(\"/\", \"\")\n",
    "        return prefix\n",
    "\n",
    "\n",
    "# 2. Apply it to all datasets\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    df[\"TicketPrefix\"] = df[\"ticket\"].apply(clean_ticket)\n",
    "\n",
    "# Let's see what we found\n",
    "print(\"Top 10 Ticket Prefixes found:\")\n",
    "print(X_train[\"TicketPrefix\"].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "### Step 2: Grouping \"Rare\" Prefixes\n",
    "\n",
    "You will likely see many prefixes that only appear once or twice (like SP, SOP). These will confuse the model.\n",
    "\n",
    "We need to group any prefix with fewer than 10 occurrences into a **\"Rare\\_Ticket\"** category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Calculate frequency of each prefix in TRAINING set\n",
    "prefix_counts = X_train[\"TicketPrefix\"].value_counts()\n",
    "\n",
    "# 2. Create a list of \"Rare\" prefixes (those appearing less than 10 times)\n",
    "# You can adjust the threshold (10) if you want\n",
    "rare_prefixes = prefix_counts[prefix_counts < 10].index\n",
    "\n",
    "# 3. Replace them with 'Rare_Ticket'\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    # If the prefix is in our \"rare\" list, rename it\n",
    "    df.loc[df[\"TicketPrefix\"].isin(rare_prefixes), \"TicketPrefix\"] = \"Rare_Ticket\"\n",
    "\n",
    "print(\"Rare tickets grouped.\")\n",
    "print(X_train[\"TicketPrefix\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "### Step 3: Drop the Original & Encode\n",
    "\n",
    "Now that we have the clean TicketPrefix column, the original ticket column is useless.\n",
    "\n",
    "**Important:** Since TicketPrefix is text (Categorical), we must add it to your One-Hot Encoding list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Drop the original messy ticket column\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    df.drop(columns=[\"ticket\"], inplace=True, errors=\"ignore\")\n",
    "\n",
    "# 2. One-Hot Encode the NEW 'TicketPrefix' column\n",
    "# We do this separately since you already encoded the other columns\n",
    "prefix_dummies_train = pd.get_dummies(X_train[\"TicketPrefix\"], prefix=\"Ticket\", drop_first=True)\n",
    "prefix_dummies_val = pd.get_dummies(X_val[\"TicketPrefix\"], prefix=\"Ticket\", drop_first=True)\n",
    "prefix_dummies_test = pd.get_dummies(X_test[\"TicketPrefix\"], prefix=\"Ticket\", drop_first=True)\n",
    "\n",
    "# 3. Join them back to your main dataframes\n",
    "X_train = pd.concat([X_train, prefix_dummies_train], axis=1)\n",
    "X_val = pd.concat([X_val, prefix_dummies_val], axis=1)\n",
    "X_test = pd.concat([X_test, prefix_dummies_test], axis=1)\n",
    "\n",
    "# 4. Drop the temporary 'TicketPrefix' text column\n",
    "X_train.drop(columns=[\"TicketPrefix\"], inplace=True)\n",
    "X_val.drop(columns=[\"TicketPrefix\"], inplace=True)\n",
    "X_test.drop(columns=[\"TicketPrefix\"], inplace=True)\n",
    "\n",
    "# 5. ALIGN COLUMNS (Crucial Step!)\n",
    "# Ensure Val/Test have exactly the same columns as Train\n",
    "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "print(f\"Ticket feature engineering complete. Total columns: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "## One hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "**The Concept:**Machine learning models are mathematical equations. They can't multiply \"male\" by 5.One-Hot Encoding creates a new binary column (0 or 1) for every category.\n",
    "\n",
    "*   sex becomes: sex\\_male (0 or 1) and sex\\_female (0 or 1).\n",
    "    \n",
    "*   embarked becomes: embarked\\_S, embarked\\_C, embarked\\_Q.\n",
    "    \n",
    "\n",
    "### The \"Hidden Trap\" (Column Mismatch)\n",
    "\n",
    "Here is where 90% of beginners crash their model.\n",
    "\n",
    "Imagine your Training set has a passenger with **Deck T**, but your Test set _doesn't_.\n",
    "\n",
    "*   X\\_train will have a column deck\\_T.\n",
    "    \n",
    "*   X\\_test will **missing** that column.\n",
    "    \n",
    "*   **Result:** The model will crash during prediction because it expects deck\\_T to exist.\n",
    "    \n",
    "\n",
    "We will fix this using a technique called **\"Aligning Columns\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "### Handling pclass First"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "Since we want to try **multiple models** to find the best one (including Linear Regression or Neural Networks), **One-Hot Encoding** is safer. It prevents the linear models from making bad assumptions.\n",
    "\n",
    "Because pandas sees 1, 2, 3 as integers, get\\_dummies will **ignore** pclass by default unless we force it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Cast Pclass to string (ONLY if it exists)\n",
    "# This prevents the KeyError if you run the cell multiple times\n",
    "for df in [X_train, X_val, X_test]:\n",
    "    if \"pclass\" in df.columns:\n",
    "        df[\"pclass\"] = df[\"pclass\"].astype(str)\n",
    "\n",
    "# 2. Define the list of columns we WANT to encode\n",
    "target_categorical_cols = [\"pclass\", \"sex\", \"embarked\", \"title\", \"deck\", \"TicketPrefix\"]\n",
    "\n",
    "# 3. Filter the list: Only encode columns that actually EXIST right now\n",
    "# This is the \"Idempotent\" magic. If 'pclass' is gone, it won't try to encode it again.\n",
    "existing_cols = [col for col in target_categorical_cols if col in X_train.columns]\n",
    "\n",
    "if existing_cols:\n",
    "    print(f\"Encoding the following columns: {existing_cols}\")\n",
    "\n",
    "    # 4. Apply One-Hot Encoding\n",
    "    X_train = pd.get_dummies(X_train, columns=existing_cols, drop_first=True)\n",
    "    X_val = pd.get_dummies(X_val, columns=existing_cols, drop_first=True)\n",
    "    X_test = pd.get_dummies(X_test, columns=existing_cols, drop_first=True)\n",
    "\n",
    "    # 5. ALIGN COLUMNS\n",
    "    X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
    "    X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "    # 6. Fix Booleans\n",
    "    for df in [X_train, X_val, X_test]:\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == \"bool\":\n",
    "                df[col] = df[col].astype(int)\n",
    "\n",
    "    print(\" One-Hot Encoding complete.\")\n",
    "else:\n",
    "    print(\"No categorical columns found. (They might already be encoded!)\")\n",
    "\n",
    "print(f\"Total Features: {X_train.shape[1]}\")\n",
    "print(X_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "## Saving the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "**Save BEFORE Scaling.**\n",
    "\n",
    "### Why? (The \"Two-Model\" Problem)\n",
    "\n",
    "You are about to enter the **Model Selection** phase where you will test different algorithms. They have different needs:\n",
    "\n",
    "1.  **Tree Models (Random Forest, XGBoost):** They **do not** need scaling. In fact, keeping the original numbers (Age=22 vs Age=0.45) makes the model easier to interpret (\"People under Age 10 survived\" vs \"People under 0.12 std deviation survived\").\n",
    "    \n",
    "2.  **Linear/Neural Models (Logistic Regression, Neural Networks):** They **REQUIRE** scaling.\n",
    "    \n",
    "\n",
    "If you save the data _after_ scaling, you \"corrupt\" the dataset for your Tree models, making them harder to explain.\n",
    "\n",
    "### The Strategy\n",
    "\n",
    "1.  **Save the Encoded (Unscaled) Data now.** This is your \"Master\" processed dataset.\n",
    "    \n",
    "2.  **Load this Master dataset** in your training notebooks.\n",
    "    \n",
    "    *   If training Random Forest -> Use as is.\n",
    "        \n",
    "    *   If training Neural Network -> Apply the scaler (which you saved as a .pkl) on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define save path\n",
    "save_dir = \"../data/04-encoded\"  # Distinct folder for this version\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# 2. Save the datasets\n",
    "X_train.to_csv(f\"{save_dir}/X_train_encoded.csv\", index=False)\n",
    "X_val.to_csv(f\"{save_dir}/X_val_encoded.csv\", index=False)\n",
    "X_test.to_csv(f\"{save_dir}/X_test_encoded.csv\", index=False)\n",
    "\n",
    "# Don't forget y! (Targets haven't changed, but good to keep them together)\n",
    "y_train.to_csv(f\"{save_dir}/y_train.csv\", index=False)\n",
    "y_val.to_csv(f\"{save_dir}/y_val.csv\", index=False)\n",
    "\n",
    "print(f\"Master Encoded Data saved to {save_dir}\")\n",
    "print(\"   (Use this for Random Forest / XGBoost)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "califonia_housing_prices",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
